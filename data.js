// Sample posts data
// Attach posts to the window object so pages that read `window.posts` work
window.posts = [
  {
    id : 1,
    title : "A Theory of Response Sampling in LLMs",
    link : "https://aclanthology.org/2025.acl-long.1454/",
    date : "2025-10-26",
    image : "image/25-10-25.png",
    citation : "[A Theory of Response Sampling in LLMs: Part Descriptive and Part Prescriptive](https://aclanthology.org/2025.acl-long.1454/) (Sivaprasad et al., ACL 2025)",
    excerpt : `A short summary about "A Theory of Response Sampling in LLMs" exploring prescriptive bias in LLM samples.`,
    post : `আমি আমার ইন্টার্নশিপের পরের তিন মাস প্রতিদিন অন্তত একটা করে গবেষণাপত্র পড়ার সংকল্প নিয়েছি। সেই পরিকল্পনার অংশ হিসেবে আজ যে পেপারটা পড়লাম, সেটা ACL 2025-এর সেরা গবেষণাপত্রের পুরস্কার পেয়েছে।

অনেকে মনে করেন AI বা LLM (Large Language Model) মানে একেবারে নিরপেক্ষ একটা বস্তু। যা জিজ্ঞেস করা হয়, সে কেবল পরিসংখ্যান অনুযায়ী সঠিক উত্তর দেয়। কিন্তু ACL 2025-এ প্রকাশিত এই গবেষণাপত্র "A Theory of Response Sampling in LLMs" দেখাচ্ছে, বিষয়টা এতটা সরল নয়।

গবেষকরা প্রথমে একটি সম্পূর্ণ কাল্পনিক শখ তৈরি করেন  ‘গ্লাবিং’। এরপর তারা AI-কে এই গ্লাবিং নিয়ে ১০০ টা ডেটা দেন, যেমন মানুষ সপ্তাহে কত ঘণ্টা গ্লাবিং করে। এই ডেটার গড় ছিল প্রায় ৪৫ ঘণ্টা। (মনে রাখবেন)

এখন তারা সেই ডেটার সঙ্গে গ্রেড (A+ থেকে D-) যুক্ত করেন, যাতে একটি মূল্যবোধ যুক্ত হয়।

১. পজিটিভ কন্ডিশন: বেশি গ্লাবিং করা ভালো
২. নেগেটিভ কন্ডিশন: কম গ্লাবিং করা স্বাস্থ্যকর

AI-কে দুটি কাজ করতে বলা হয়,  গড় কত বল এবং একটি নমুনা দাও। গড়ের ক্ষেত্রে সে সঠিকভাবেই ৪৫ জানায়। কিন্তু নমুনা দেওয়ার সময় দেখা গেল তার আচরণ বদলে গেছে।

যখন বলা হয়েছিল বেশি গ্লাবিং ভালো, AI নমুনা দিয়েছে ৪৬.৭ ঘণ্টা।

যখন বলা হয়েছিল কম গ্লাবিং ভালো, সে দিয়েছে মাত্র ৩৬.৫ ঘণ্টা।

অর্থাৎ AI শুধু তথ্য বিশ্লেষণ করছে না, বরং ব্যবহারকারীর দেওয়া মূল্যবোধের দিকেও ঝুঁকছে। তথ্যের বদলে আদর্শের সঙ্গে সামঞ্জস্য রাখার চেষ্টা করছে।

গবেষকরা দেখিয়েছেন, এই prescriptive bias অনেক ক্ষেত্রে ঝুঁকিপূর্ণ হতে পারে। উদাহরণ হিসেবে একটি মেডিকেল পরিস্থিতি দেওয়া হয়েছে। যদি একজন AI কে জিজ্ঞেস করেন, “এই রোগে সুস্থ হতে গড়ে কতদিন লাগে”, আর AI বাস্তব গড়ের বদলে “ভালো শোনায়” এমন কম সময় বলে ফেলে, তাহলে চিকিৎসা পরিকল্পনা ও ব্যবস্থাপনায় বিপর্যয় ঘটতে পারে। সত্য বলার বদলে AI তখন এমন উত্তর দিতে চায় যা আদর্শ মনে হয়।

গবেষণাপত্রটি কোনো কোডিং ভিত্তিক সমাধান দেয়নি, বরং এই আচরণ বোঝার জন্য একটি তাত্ত্বিক কাঠামো দিয়েছে। 

গবেষকদের মতে, AI-এর এই আদর্শ ঝোঁক কোথা থেকে আসে, তা বোঝা নির্ভরযোগ্য ও নৈতিকভাবে নিরপেক্ষ AI তৈরি করার প্রথম ধাপ। যদি বোঝা যায় কখন AI অতিরিক্ত নৈতিক হতে চায়, তখন তাকে শুধুমাত্র তথ্যভিত্তিক হতে শেখানো সম্ভব।

তবে গবেষকরা এও স্বীকার করেছেন যে, AI এই আচরণ শেখে ঠিক কোথা থেকে, তা এখনও স্পষ্ট নয়। এটি কি তার বিশাল ট্রেনিং ডেটা থেকে, নাকি মানুষের দেওয়া ফিডব্যাক ট্রেনিং (RLHF) থেকে এসেছে, তা এখনো অনিশ্চিত।

এই গবেষণা প্রমাণ করেছে, AI শুধু যান্ত্রিক তোতাপাখি না। তারও আছে আচরণগত বিশেষভাবধারা, নৈতিক চিন্তাভাবনা, আর মাঝে মাঝে এক ধরনের আত্মসন্তুষ্টি যে সে "ভালো" কিছু বলছে।`,
    tags : ["LLM","AI","Research Paper Summary","ACL2025"],
  },

  {
    id : 2,
    title : "Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral",
    link : "https://aclanthology.org/2025.acl-long.294/",
    date : "2025-10-27",
    image : "image/26-10-25.png",
    citation : "[Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral](https://aclanthology.org/2025.acl-long.294/) (Kumar & Jurgens, ACL 2025)",
    excerpt : `A short summary about multilingual moral reasoning in AI and its cultural dependencies.`,
    post : `আগামী ৩ মাস প্রতিদিন একটা করে গবেষণাপত্র পড়ার লক্ষ্যের আজ ২য় দিন। আজকের গবেষণাপত্রটি ২০২৫ সালে ACL এ সেরা রিসোর্স পেপার হিসাবে পূরষ্কার পেয়েছে। বেশ ইউনিক একটা জিনিস নিয়ে কাজ করেছেন গবেষকরা, তবে আমার মতে এই কাজ প্রচণ্ড পরিশ্রমসাপেক্ষ। যাইহোক এখন আসল আলাপে যাই,

সঠিক বা ভুলের ধারণা কি সব দেশে এক? আমরা যাকে 'নৈতিক' বলে মনে করি, একজন চীনা বা রুশ নাগরিকও কি ঠিক সেভাবেই ভাবেন? আর কৃত্রিম বুদ্ধিমত্তা বা AI কি মানুষের এই জটিল নৈতিকতার পার্থক্য বোঝে? 

Are Rules Meant to be Broken? Understanding Multilingual Moral  as a Computational Pipeline with UNIMORAL নামের গবেষণাপত্রটা এই বিষয়েই কথা বলে।

এই গবেষণায় 'ইউনিমোরাল' নামে বিশাল একটা ডেটাসেট তৈরি করা হয়েছে। গবেষকরা ছয়টি ভিন্ন ভাষার (ইংরেজি, আরবি, চীনা, হিন্দি, রুশ এবং স্প্যানিশ) মানুষের কাছে নানা ধরনের নৈতিক সংকট তুলে ধরেন। 

এর মধ্যে যেমন ক্লাসিক মনস্তাত্ত্বিক প্রশ্ন ছিল, তেমনই রেডিট-এর মতো সোশ্যাল মিডিয়া থেকে নেওয়া বাস্তব জীবনের দ্বিধাও ছিল। অংশগ্রহণকারীরা শুধু কোন পদক্ষেপটি বেছে নিলেন তাই নয়, কেন নিলেন, কোন কোন বিষয় (যেমন আবেগ, সংস্কৃতি, আইন বা দায়িত্ববোধ) তাদের সিদ্ধান্তকে প্রভাবিত করল, সে সম্পর্কেও বিস্তারিত তথ্য দিয়েছেন।

এরপর গবেষকরা বর্তমানের জনপ্রিয় এআই মডেলগুলোকে একই পরীক্ষায় ফেলেন। ফলাফল বেশ চমকে দেওয়ার মত যদিও খুব একটা অপ্রত্যাশিত না।

প্রথমত, দেখা গেছে এআই মডেলগুলো মানুষের মতো নৈতিক বিচার করতে প্রায় পুরোপুরি ব্যর্থ। বেশিরভাগ ক্ষেত্রেই তাদের পারফরম্যান্স ছিল কেবলই আন্দাজে ঢিল ছোঁড়ার মতো। 

দ্বিতীয়ত, গবেষণায় স্পষ্টভাবে প্রমাণিত হয়েছে যে, নৈতিকতা মোটেও সর্বজনীন নয়। এটি সংস্কৃতির ওপর ভীষণভাবে নির্ভরশীল। যেমন, আরবি ও হিন্দিভাষী মানুষেরা সিদ্ধান্ত নেওয়ার ক্ষেত্রে 'পবিত্রতা' বা 'ঐশ্বরিক মূল্যবোধ'-কে যতটা গুরুত্ব দিয়েছেন, স্প্যানিশভাষীরা ততটা দেননি; তারা বরং 'সবার জন্য কোনটা ভালো' সেই দৃষ্টিকোণ থেকে ভেবেছেন।

সবচেয়ে গুরুত্বপূর্ণ যে বিষয়টি উঠে এসেছে তা হলো, এআই মডেলগুলো ইংরেজি, স্প্যানিশ বা রুশ ভাষায় তুলনামূলক ভালো করলেও আরবি এবং হিন্দির মতো ভাষায় খুবই দুর্বল পারফর্ম করেছে। এর প্রধান কারণ হলো, এই ভাষাগুলোয় প্রশিক্ষণের জন্য ডেটার মারাত্মক অভাব।

এই গবেষণার সিদ্ধান্ত খুবই পরিষ্কার: কৃত্রিম বুদ্ধিমত্তাকে যদি সত্যিই মানুষের জন্য কার্যকরী এবং 'নৈতিক' করে তুলতে হয়, তবে তাকে শুধু ইংরেজি বা পশ্চিমা সংস্কৃতি শেখালেই চলবে না। তাকে বিশ্বের প্রতিটি ভাষার নিজস্ব সাংস্কৃতিক ও নৈতিক মূল্যবোধ সম্পর্কে সচেতন করে তুলতে হবে। এই গবেষণাটি সেই লক্ষ্যেই একটি যুগান্তকারী পদক্ষেপ।`,
    tags : ["AI","Ethics","Multilingual","Research Paper Summary","ACL2025"],
  },

  {
    id : 3,
    title : "Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest",
    link : "https://aclanthology.org/2023.acl-long.41/",
    date : "2025-10-28",
    image : "image/27-10-25.png",
    citation : "[Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest](https://aclanthology.org/2023.acl-long.41/) (Hessel et al., ACL 2023)",
    excerpt : `A short summary about AI's ability to understand humor through New Yorker cartoon captions.`,
    post : `প্রতিদিন একটি গবেষণাপত্র, আজ তৃতীয় দিন। দুইদিন AI safety নিয়ে দেখার পর হটাৎ মনে হল, জিপিটি যে জোক বলে, সেইটা কি সে নিজে বুঝে?  নাকি ঐ হাসির ইমোজিই শেষ? তো জিনিসটা নিয়ে কেউ কাজ করেছে কিনা খুজতে গিয়ে এই পেপারটা পেলাম৷ ২০২৩ এ ACL এর অন্যতম সেরা গবেষণাপত্রের পুরষ্কার পেয়েছিল পেপারটা। তো যাইহোক আসল কথায় ফিরে যাই,

কম্পিউটার কি আসলেই জোকস বোঝে? নাকি পুরোটাই অভিনয়? 

চ্যাটজিপিটির মতো এ-আই কথা বলতে পারে, ছবি আঁকতে পারে, কোড লিখতে পারে। কিন্তু একটা প্রশ্ন থেকেই যায়, এই এআই কি মানুষের মতো করে কৌতুক বা হিউমার জিনিসটা বোঝে? একটা জোকস শুনে সে কি আমাদের মতো 'মজা' পায়?

এই জটিল প্রশ্নটার উত্তর খুঁজতেই গবেষকরা দারুণ একটা কাজ করেছেন। তারা সোজা চলে গেছেন আমেরিকার বিখ্যাত "নিউ ইয়র্কার" ম্যাগাজিনের কাছে। এই ম্যাগাজিনটি তাদের কার্টুনের জন্য পৃথিবী বিখ্যাত। কিন্তু তাদের কার্টুনের মজাটা খুব একটা সরল না, বরং খুব গভীর আর প্যাঁচানো। অনেক সময় ছবির সাথে ক্যাপশনের কোনো মিলই থাকে না, পুরোটাই একটা অদ্ভুত পরিস্থিতি তৈরি করে, যা আমাদের হাসায়। এই ধরনের হিউমার বুঝতে হলে শুধু ভাষা জানলে চলে না, সমাজের রীতিনীতি, সংস্কৃতি আর জীবনের নানা ঝুট-ঝামেলা বোঝার ক্ষমতা থাকতে হয়।

গবেষকরা ঠিক এই কঠিন পরীক্ষাটিই এআইকে দিয়ে করিয়েছেন। তারা এআই এর রসবোধ বোঝার ক্ষমতাকে ধাপে ধাপে চ্যালেঞ্জ করেছেন।

প্রথম ধাপে, এআইকে একটা কার্টুন ছবি দেখানো হয়েছে। সাথে দেওয়া হয়েছে পাঁচটি ভিন্ন ভিন্ন ক্যাপশন। এআইকে বলতে হয়েছে, এর মধ্যে কোনটা আসল ক্যাপশন, যেটা কার্টুনিস্ট ভেবেছিলেন। এটা হলো বেসিক ম্যাচিং টেস্ট।

দ্বিতীয় ধাপে, কাজটা আরও কঠিন করা হলো। একটা কার্টুনের সাথে এবার দুটো ক্যাপশন দেওয়া হলো। একটা হলো সেই কার্টুনের জন্য সাধারন জনগনের ভোটে বিজয়ী ক্যাপশন৷  অন্যটা ছিল একটা একেবারেই সাধারণ মানের ক্যাপশন। এআইকে বলা হলো, এই দুইটার মধ্যে কোনটা বেশি ভালো বা বেশি মজার, সেটা বেছে বের করো। এটা হলো হিউমারের 'মান' বোঝার পরীক্ষা।

তৃতীয় এবং সবচেয়ে কঠিন ধাপে, এআইকে একটা কার্টুন আর সেটার বিজয়ী ক্যাপশনটা দিয়ে বলা হলো, আচ্ছা, এবার বুঝিয়ে বলো তো, এই ক্যাপশনটা এই ছবির সাথে কেন এত মজার? কী এমন আছে এতে যে মানুষ হাসছে? এটা হলো জোকস 'ব্যাখ্যা' করার পরীক্ষা, যা করতে হলে রসবোধের একেবারে গভীরে ঢুকতে হয়।

মজার ব্যাপার হলো, গবেষকরা এই পরীক্ষাগুলো দুইভাবে নিয়েছেন। একবার তারা এআই মডেলকে সরাসরি কার্টুনের ছবিটিই দেখিয়েছেন। আরেকবার তারা এআই এর কাজটা একটু সহজ করে দিয়েছেন। তারা নিজেরা মানুষ দিয়ে কার্টুনের ছবিটা কেমন, তাতে কী কী অদ্ভুত জিনিস আছে, তার একটা বিস্তারিত লিখিত বর্ণনা তৈরি করেছেন। যেমন, "একটা লোক স্যুটেড-বুটেড হয়ে অফিসে বসে আছে, কিন্তু তার টেবিলে কম্পিউটারের জায়গায় একটা আস্ত ভেড়া দাঁড়িয়ে আছে।" 

এই বর্ণনাটা তারা জিপিটি-৪ এর মতো শক্তিশালী ল্যাঙ্গুয়েজ মডেলকে দিয়ে তারপর ক্যাপশনটা দিয়েছেন। এর উদ্দেশ্য ছিল এটা দেখা যে, ছবি বোঝার কষ্টটা যদি আমরা বাদও দিই, শুধু লেখা পড়েই কি এআই জোকসের মজাটা ধরতে পারে?

ফলাফল যা এসেছে তা খুবই পরিষ্কার। রসবোধের এই পরীক্ষায় এআই মানুষের চেয়ে হাজার হাজার মাইল পিছিয়ে আছে।

দেখা গেছে, প্রথম ধাপে যেখানে মানুষ ৯৪ শতাংশ ক্ষেত্রেই সঠিক ক্যাপশনটা খুঁজে বের করতে পেরেছে, সেখানে সেরা এআই মডেলগুলো সফল হয়েছে মাত্র ৬২ শতাংশ ক্ষেত্রে।

কিন্তু সবচেয়ে অবাক করা ফলাফল এসেছে শেষ ধাপে। যখন এআইকে কার্টুনের পুরো বর্ণনা হাতে-কলমে লিখে দেওয়া হলো, তারপরেও জোকস ব্যাখ্যা করার ক্ষেত্রে জিপিটি-৪ মানুষের কাছে পাত্তাই পায়নি। ৬৮ শতাংশ ক্ষেত্রেই মানুষ, এআই এর লেখা ব্যাখ্যার চেয়ে মানুষের করা ব্যাখ্যাকেই বেশি ভালো, যৌক্তিক এবং মজার বলে উল্লেখ করেছে।

এই গবেষণাটা একটা জিনিস খুব পরিষ্কার করে। এআই হয়তো শব্দ চেনে, বাক্য গঠন করতে পারে, তথ্য সাজিয়ে দিতে পারে। কিন্তু মানুষের রসবোধের পেছনে যে বিশাল সাংস্কৃতিক জ্ঞান, অভিজ্ঞতা, আর অসংগতি বোঝার ক্ষমতা কাজ করে, সেই 'বোঝা' নামক জিনিসটা কম্পিউটারের এখনও হয়নি। সে হয়তো বলতে পারে কোনটা 'ফানি' বলে ডেটাবেজে লেখা আছে, কিন্তু সে নিজে সেই মজাটা 'অনুভব' করতে পারে না。

* যারা শেষ পর্যন্ত পড়েছেন। আপনারাও চাইলে চ্যালেঞ্জটায় অংশ নিতে পারেন। ব্যস্ততার জন্য দিনে একটা না পারেন সপ্তাহে একটা পড়লেন। সেইটা নিয়েই নাহয় লিখলেন। `,
    tags : ["AI","Humor","Research Paper Summary","ACL2023"],
  },

  {
    id : 4,
    title : "Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs",
    link : "https://aclanthology.org/2025.acl-long.341/",
    date : "2025-10-29",
    image : "image/28-10-25.png",
    citation : "[Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs](https://aclanthology.org/2025.acl-long.341/) (Wang et al., ACL 2025)",
    excerpt : `A short summary about fairness and difference awareness in LLMs.`,
    post : `#A_Paper_A_Day এর আজকে চতুর্থ দিন। আজকে ২০২৫ এর ACL এ আসা স্ট্যানফোর্ডের একটা award winning paper পড়লাম। বাস্তব দুনিয়া যে কতটা কঠিন এবং বিভক্ত আর কাল্পনিক সাম্য যে কত ভঙ্গুর তা এই গবেষণায় হাড়ে হাড়ে প্রমাণ হয়ে যায়。

কৃত্রিম বুদ্ধিমত্তা কতটা ‘নিরপেক্ষ’ (Fair) হওয়া উচিত, তা নিয়ে আজকাল অনেক কথা হয়। সাধারণত আমরা মনে করি, একটা ভালো এআই সিস্টেম সব মানুষকে এক চোখে দেখবে, কোনো পার্থক্য করবে না। একে বলে ‘কালার ব্লাইন্ড’ বা পার্থক্য না করা নীতি।

কিন্তু সম্প্রতি স্ট্যানফোর্ডের গবেষকদের এই পেপার ধারণাটিকে পুরোপুরি চ্যালেঞ্জ করেছে। পেপারটির নাম "ফেয়ারনেস থ্রু ডিফারেন্স অ্যাওয়ারনেস"।

এই গবেষণার মূল কথা হলো, সব ক্ষেত্রে সবার সাথে সমান আচরণ করাই নিরপেক্ষতা নয়। বরং কিছু কিছু ক্ষেত্রে, সত্যিকারের নিরপেক্ষতা বা সুবিচারের জন্য বিভিন্ন গোষ্ঠীর মধ্যকার বাস্তব পার্থক্যকে স্বীকার করাটা জরুরি। একেই গবেষকরা বলছেন ‘ডিফারেন্স অ্যাওয়ারনেস’ বা ‘পার্থক্য সচেতনতা’।

ব্যাপারটা কী?

ধরেন, আইনি বা মেডিকেল বিষয়ে। কিছু আইন হয়তো শুধু নির্দিষ্ট লিঙ্গের মানুষের জন্য প্রযোজ্য, অথবা কোনো রোগের ঝুঁকি হয়তো নির্দিষ্ট জাতিগোষ্ঠীর মধ্যে বেশি। একটা এআই যদি এই পার্থক্যগুলো ‘জানে’ কিন্তু সেগুলোকে ‘অস্বীকার’ করে সবার সাথে সমান আচরণ করার ভান করে, তবে তা সুবিচারের বদলে ক্ষতিই বেশি করবে। 

আবার, কোনো একটা স্টেরিওটাইপ বা গৎবাঁধা কথা একটা গোষ্ঠীর জন্য যতটা ক্ষতিকর, অন্য গোষ্ঠীর জন্য হয়তো ততটা নয়। এই পার্থক্যটা বুঝতে পারাটাও জরুরি। পেপারের Abstract এই এর একটা উদাহরণ আছে। একজন মুসলিম কে অজথা আতঙ্কসৃষ্টিকারী বলা যতটা ক্ষতিকর এই সমাজে, একজন নারীকে সেই একই কথা বলা হয়ত ততটা ক্ষতিকর না। একটা খুবই গভীর স্টেরিওটাইপ আর অন্যটক হয়ত বেশিরভাগ ক্ষেত্রেই মজারছলে বলা মনে হবে। 

আগের সিস্টেমগুলোর সমস্যা ছিল যে, সেগুলোকে ‘নিরপেক্ষ’ বানানোর পরীক্ষায় পাস করার জন্য তারা সব ধরনের পার্থক্য করা বন্ধ করে দিত। ফলে তারা বাস্তব পৃথিবীর জটিলতা বুঝতে পারতো না।

গবেষকরা এটা মাপার জন্য দারুণ একটি পদ্ধতি তৈরি করেছেন।

গবেষকরা মোট ১৬,০০০ প্রশ্নের এক বিশাল ডেটাসেট বা পরীক্ষার প্রশ্নপত্র তৈরি করেছেন। তবে এর আগে তারা একটা গুরুত্বপূর্ণ কাজ করেছেন। তারা ‘নিরপেক্ষতা’ মাপার প্রশ্নগুলোকে দুই ভাগে ভাগ করেছেন:

১. বর্ণনামূলক (Descriptive): এই প্রশ্নগুলো বাস্তব তথ্যের ওপর ভিত্তি করে। যেমন, কোনো দেশে নির্দিষ্ট ধর্মের মানুষ কত শতাংশ, বা কোনো নির্দিষ্ট পেশায় কোন গোষ্ঠীর মানুষ বেশি আছে। এখানে এআই এর কাজ হলো সত্যটা বলা。

২. নীতিগত (Normative): এই প্রশ্নগুলো মূল্যবোধ বা আদর্শের ওপর ভিত্তি করে। যেমন, কোনো পিছিয়ে পড়া গোষ্ঠীর জন্য ‘অ্যাফারমেটিভ অ্যাকশন’ (বিশেষ সুবিধা) দরকার কি না, অথবা দুটি গৎবাঁধা কথার মধ্যে কোনটি বেশি ক্ষতিকর।

এই প্রশ্নপত্র তৈরির পর তারা দুটি নতুন জিনিস মেপেছেন,

১. ডিফঅ্যাওয়ার (DiffAware) বা পার্থক্য সচেতনতা: যখন দুটি গোষ্ঠীর মধ্যে আসলেই কোনো বাস্তব, যৌক্তিক বা আইনি পার্থক্য আছে (যেমনটা বর্ণনামূলক প্রশ্নে দেওয়া আছে), তখন এআই কি সেই পার্থক্যটা ধরতে পারছে?

২. কন্টেক্সটঅ্যাওয়ার (CtxtAware) বা প্রেক্ষাপট সচেতনতা: এআই কি এটা বুঝতে পারছে যে, কখন পার্থক্য করাটা জরুরি আর কখন জরুরি নয়? অর্থাৎ, সে কি শুধু দরকারি জায়গাতেই পার্থক্য করছে, নাকি অপ্রয়োজনেও করছে?

ফলাফলগুলো বেশ চমৎকার 

প্রথমত, যেসব এআই মডেল (যেমন GPT-4 or Gemma) পুরাতন ‘নিরপেক্ষতার’ পরীক্ষাগুলোতে প্রায় নিখুঁত স্কোর পায়, সেগুলো এই নতুন ও জটিল পরীক্ষায় খুব খারাপ করেছে। এর মানে, আমাদের বর্তমান ‘ফেয়ার’ মডেলগুলো আসলে বাস্তব পার্থক্য বুঝতে অক্ষম।

দ্বিতীয়ত, গবেষকরা দেখেছেন, একটা মডেল যত ‘স্মার্ট’ বা শক্তিশালী হচ্ছে, তার ‘প্রেক্ষাপট সচেতনতা’ (কখন পার্থক্য করতে হবে) তত বাড়ছে। কিন্তু তার ‘পার্থক্য সচেতনতা’ (কী কী পার্থক্য আছে) মোটেও বাড়ছে না। অর্থাৎ, মডেলগুলো বড় হলেই যে এই বাস্তব জ্ঞান নিজে নিজে শিখে ফেলবে, তা নয়।

সবচেয়ে ভয়ের ব্যাপার হলো তৃতীয় ফলাফলটি। যখন গবেষকরা মডেলগুলোকে ‘নিরপেক্ষ’ করার জন্য বর্তমানে প্রচলিত পদ্ধতিগুলো ব্যবহার করলেন (যেমন, মডেলকে নির্দেশ দেওয়া হলো ‘তুমি কোনো বায়াস দেখাবে না’ বা ‘সবার সাথে সমান আচরণ করো’), তখন ফলাফল আরও খারাপ হলো। 

মডেলগুলো সত্যিকারের ‘পার্থক্য সচেতনতা’ আরও বেশি হারিয়ে ফেলল। এমনকি তারা বাস্তব সত্যকেও অস্বীকার করা শুরু করল, শুধু ‘নিরপেক্ষ’ থাকার ভান করার জন্য। যেমন, কোনো পেশায় নারীরা সংখ্যাগুরু হলেও, মডেল সেটা অস্বীকার করে বলতে শুরু করল যে নারী-পুরুষ আসলে সমান।

এই পেপারটি দেখাচ্ছে যে, এআই কে ‘নিরপেক্ষ’ বানানোর আমাদের বর্তমান চেষ্টা বড্ড বেশি সরল। আমরা এআই কে এমন একটা পৃথিবী শেখাচ্ছি যেখানে কোনো পার্থক্য নেই। কিন্তু বাস্তব পৃথিবীটা তেমন নয়। সত্যিকারের সুবিচারের জন্য কখনও কখনও পার্থক্যগুলো জানা এবং সে অনুযায়ী ব্যবস্থা নেওয়া জরুরি। এই গবেষকরা সেই জটিল বাস্তবতাকে মাপার একটা নতুন পথ দেখালেন।`,
    tags : ["AI","Fairness","Ethics","Research Paper Summary","ACL2025"],
  },

  {
    id : 5,
    title : "Language Models Resist Alignment: Evidence From Data Compression",
    link : "https://aclanthology.org/2025.acl-long.1141/",
    date : "2025-10-30",
    image : "image/29-10-25.png",
    citation : "[Language Models Resist Alignment: Evidence From Data Compression](https://aclanthology.org/2025.acl-long.1141/) (Ji et al., ACL 2025)",
    excerpt : `A short summary about why AI models resist alignment and return to their original behavior.`,
    post : `আজকে পঞ্চম দিন। আলহামদুলিল্লাহ এখন পর্যন্ত প্রতিদিন একটা করে গবেষণাপত্র পড়ার লক্ষ্যে টিকে থাকতে পেরেছি। চেষ্টা করব যেন এটা চালিয়ে যেতে পারি ইনশাআল্লাহ। আজকে যেই গবেষণাপত্র পড়লাম সেইটা খুবই মজার একটা বিষয় নিয়ে আলোচনা করেছে, এবং এজন্য Outstanding গবেষণাপত্র হিসাবে ACL 2025 এ পুরষ্কারও পেয়েছে। চ্যাটজিপিটি বা অন্যান্য মডেলগুলো কেন মাঝে মাঝে 'উল্টাপাল্টা' কথা বলে?

আমরা যারা বিভিন্ন AI ব্যবহার করি, তারা জানি যে এই মডেলগুলোকে অনেক 'ভদ্র' এবং 'নিরাপদ' করে তোলার চেষ্টা করা হয়। তাইতো জিপিটিকে আপনার দুষ্টু গার্লফ্রেন্ড বানানো এত সহজ না। কিন্তু তারপরেও অনেক সময় দেখা যায়, সামান্য চেষ্টা করতেই বা একটু ঘুরিয়ে প্রশ্ন করলেই তাদের আসল রূপ বেরিয়ে আসে, তারা আপত্তিকর বা অদ্ভুত উত্তর দিয়ে ফেলে।

কেন এমন হয়? কেন এই 'ভালো' আচরণের মুখোশটা এতই পাতলা? 

সম্প্রতি একদল গবেষক এই রহস্যের পেছনের কারণ খুঁজে পেয়েছেন। তারা এই ব্যাপারটাকে বলছেন 'ইলাস্টিসিটি' (Elasticity) বা স্থিতিস্থাপকতা। ক্লাসিকাল পদার্থবিজ্ঞানে যে স্থিতিস্থাপকতা আমরা সবাই পড়েছি, মডেলের এই আচরণের সাথে ঐ স্থিতিস্থাপকতার খুবই মিল আছে বলে মনে করেন গবেষকরা। ব্যাপারটা একদম ঠিক একটা রাবার ব্যান্ড বা স্প্রিংয়ের মতো।

ধরুন, একটা স্প্রিংকে আপনি টেনে লম্বা করলেন। এটা হলো এআই মডেলকে 'ভালো' বা 'নিরাপদ' উত্তর শেখানো। কিন্তু ছেড়ে দিলেই স্প্রিংটা যেমন আবার আগের জায়গায় ফিরে যেতে চায়, এআই মডেলগুলোও ঠিক তেমনি তাদের পুরনো 'অশিক্ষিত' এবং অসভ্য অবস্থায় ফিরে যেতে চায়। 

গবেষকরা তাদের পেপারে দেখিয়েছেন এই অদ্ভুত আচরণের পেছনের ধাপগুলো। তারা আসলেই পরিমাপ করেছেন কখন কিভাবে একটা মডেলকে এই পর্যায়ে নিয়ে যাওয়া যায় যেন সে তার শেখা সব সভ্যতা ভুলে যায়।

চলুন দেখি গবেষকরা আসলে কি কি ধাপ চিহ্নিত করেছেন,

প্রথমতই বুঝতে হবে, একটা এআই মডেলকে যখন প্রথম তৈরি করা হয়, তখন তাকে ইন্টারনেটের প্রায় পুরোটা, মানে হাজার হাজার কোটি শব্দ আর লেখা পড়তে দেওয়া হয়। বই, আর্টিকেল, ওয়েবসাইট, ফোরাম কী নাই সেখানে! ভালো, মন্দ, অদ্ভুত সব ধরনের তথ্য সে এখান থেকে শেখে। এটাই তার 'আসল' বা প্রাথমিক অবস্থা। কারন মানুষকে তো আর ভাল হিসাবে প্রোগ্রাম করে দেওয়া হয়নি। মানুষ তার ইন্টারনেটে ভাল, খারাপ, কম ভাল, কম খারাপ সবই রেখেছে। 

দ্বিতীয়ত, মডেলটাকে ব্যবহারোপযোগী করার জন্য, তাকে আলাদা করে কিছু 'ভালো' ও 'নিরাপদ' ডেটা দিয়ে শেখানো হয়। যেমন, তাকে শেখানো হয় যে হিংসা প্রকাশ পায় এমন কথা বলা যাবে না, বা ভুল তথ্য দেওয়া চলবে না। গবেষকরা বলছেন, এই 'ভালো' বানানোর ডেটা, আগের সেই বিশাল ইন্টারনেট ডেটার তুলনায় খুবই সামান্য, সাগরের মধ্যে এক ফোঁটা পানির মতো।

তৃতীয়ত, গবেষকরা তাদের গবেষণায় 'কম্প্রেশন থিওরি' বা তথ্য সংকুচিত করার একটা তত্ত্ব ব্যবহার করেছেন। তারা দেখিয়েছেন যে, এআই মডেলগুলো শেখার সময় আসলে তথ্যকে 'সংকুচিত' করে। যে তথ্য বা ডেটার পরিমাণ যত বেশি, মডেল সেটার ওপর তত বেশি গুরুত্ব দেয়। এইটা খুব একটা অপরিচিত কথা না। সাধারন মেশিনলার্নিং এর ক্ষেত্রে আমরা এটাকে বায়াস বলে থাকি।

যেহেতু 'ভালো' ডেটার পরিমাণ খুবই কম, আর অপরিশোধিত ইন্টারনেট ডেটার পরিমাণ বিশাল, তাই মডেলের মূল চরিত্র আসলে ওই ইন্টারনেট ডেটা দিয়েই তৈরি হয়। 'ভালো' আচরণটা একটা ওপর ওপর লাগানো প্রলেপের মতো থাকে। মানে হল যে একটা দুদ্ধর্ষ মিশ্র আচরণের বুদ্ধিমত্তার মুখের উপর 'দেখো আমি কত্তো ইনোসেন্ট' এর একটা মুখোশ বা মেকাপ লাগানো।

তাহলে তারা যে পরীক্ষা করলেন, সেই পরীক্ষায় কী পেলেন?

গবেষকরা কয়েকটি অবাক করা জিনিস খুঁজে পেয়েছেন।

প্রথমেই প্রতিরোধ করা। তারা দেখেছেন, একটা অপরিশোধিত এআই মডেলকে (যে শুধু ইন্টারনেট থেকে শিখেছে, এখনোও ''ভাল'' হওয়ার মুখোশ পরানো হয়নি) 'ভালো' বানানো যত কঠিন, তার চেয়ে অনেক বেশি সহজ একটা 'ভালো' মডেলকে আবার 'খারাপ' অবস্থায় ফিরিয়ে নেওয়া। মডেলটা নিজেই যেন 'ভালো' হতে চায় না, সে তার পুরনো অবস্থায় থাকতে চায়।

দ্বিতীয়ত, দ্রুত ফিরে আসা (রিবাউন্ড)। আরও মজার ব্যাপার হলো, একটা মডেলকে আপনি যত বেশি 'নিরাপদ' বা 'ভদ্র' বানানোর চেষ্টা করবেন, সেটা তত বেশি ইলাস্টিক হয়ে ওঠে। অর্থাৎ, সামান্য একটু সুযোগ পেলেই (যেমন কিছু উল্টোপাল্টা ডেটা পেলেই) সেটা অবিশ্বাস্য দ্রুতগতিতে তার সেই পুরনো 'অনিরাপদ' অবস্থায় ফিরে যায়। স্প্রিংটা যত জোরে টেনে ধরবেন, ছাড়লে তো তত জোরেই সেটা আগের জায়গায় ফিরবে। তাই না কি?

তৃতীয়ত মডেল যত বড়, সমস্যা তত গভীর। সবচেয়ে ভয়ের ব্যাপার হলো, গবেষকরা দেখেছেন, এআই মডেল যত বড় আর শক্তিশালী হচ্ছে (যেমন জিপিটি-৪ বা তার চেয়েও উন্নত মডেল), তাদের এই 'ইলাস্টিসিটি' বা আগের অবস্থায় ফিরে যাওয়ার প্রবণতা তত বেশি বাড়ছে।

এর মানে কী?

এর সহজ মানে হলো, এআই মডেলগুলোকে শুধু ওপর থেকে 'ভালো' আচরণের একটা প্রলেপ দিয়ে ছেড়ে দেওয়াটা খুব ঝুঁকিপূর্ণ। এটা একটা মুখোশের মতো, যা যেকোনো সময় খসে পড়তে পারে। এই গবেষণাটা আমাদের চোখে আঙুল দিয়ে দেখিয়ে দিলো যে, এআইকে নিরাপদ করতে হলে আমাদের আরও গভীরের কোনো উপায় খুঁজতে হবে, যা মডেলগুলোকে শুধু বাইরে থেকে নয়, একেবারে ভেতর থেকে পরিবর্তন করতে পারবে। শুধু মুখে একটা ভালোর প্রলেপ লাগিয়ে ছেড়ে দিবে না।`,
    tags : ["AI","Alignment","Safety","Research Paper Summary","ACL2025"],
  },

  

];

// Author information
window.author = {
  name: "Adib Sakhawat",
  affiliation: "Islamic University of Technology",
  image: "https://avatars.githubusercontent.com/u/111243915?v=4"
};
