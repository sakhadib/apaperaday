// Sample posts data
// Attach posts to the window object so pages that read `window.posts` work
window.posts = [
  {
    id : 1,
    title : "A Theory of Response Sampling in LLMs",
    link : "https://aclanthology.org/2025.acl-long.1454/",
    date : "2025-10-26",
    citation : "[Are Rules Meant to be Broken? Understanding Multilingual Moral Reasoning as a Computational Pipeline with UniMoral](https://aclanthology.org/2025.acl-long.294/) (Kumar & Jurgens, ACL 2025)",
    excerpt : `A short summary about "A Theory of Response Sampling in LLMs" exploring prescriptive bias in LLM samples.`,
    post : `আমি আমার ইন্টার্নশিপের পরের তিন মাস প্রতিদিন অন্তত একটা করে গবেষণাপত্র পড়ার সংকল্প নিয়েছি। সেই পরিকল্পনার অংশ হিসেবে আজ যে পেপারটা পড়লাম, সেটা ACL 2025-এর সেরা গবেষণাপত্রের পুরস্কার পেয়েছে।

অনেকে মনে করেন AI বা LLM (Large Language Model) মানে একেবারে নিরপেক্ষ একটা বস্তু। যা জিজ্ঞেস করা হয়, সে কেবল পরিসংখ্যান অনুযায়ী সঠিক উত্তর দেয়। কিন্তু ACL 2025-এ প্রকাশিত এই গবেষণাপত্র "A Theory of Response Sampling in LLMs" দেখাচ্ছে, বিষয়টা এতটা সরল নয়।

গবেষকরা প্রথমে একটি সম্পূর্ণ কাল্পনিক শখ তৈরি করেন  ‘গ্লাবিং’। এরপর তারা AI-কে এই গ্লাবিং নিয়ে ১০০ টা ডেটা দেন, যেমন মানুষ সপ্তাহে কত ঘণ্টা গ্লাবিং করে। এই ডেটার গড় ছিল প্রায় ৪৫ ঘণ্টা। (মনে রাখবেন)

এখন তারা সেই ডেটার সঙ্গে গ্রেড (A+ থেকে D-) যুক্ত করেন, যাতে একটি মূল্যবোধ যুক্ত হয়।

১. পজিটিভ কন্ডিশন: বেশি গ্লাবিং করা ভালো
২. নেগেটিভ কন্ডিশন: কম গ্লাবিং করা স্বাস্থ্যকর

AI-কে দুটি কাজ করতে বলা হয়,  গড় কত বল এবং একটি নমুনা দাও। গড়ের ক্ষেত্রে সে সঠিকভাবেই ৪৫ জানায়। কিন্তু নমুনা দেওয়ার সময় দেখা গেল তার আচরণ বদলে গেছে।

যখন বলা হয়েছিল বেশি গ্লাবিং ভালো, AI নমুনা দিয়েছে ৪৬.৭ ঘণ্টা।

যখন বলা হয়েছিল কম গ্লাবিং ভালো, সে দিয়েছে মাত্র ৩৬.৫ ঘণ্টা।

অর্থাৎ AI শুধু তথ্য বিশ্লেষণ করছে না, বরং ব্যবহারকারীর দেওয়া মূল্যবোধের দিকেও ঝুঁকছে। তথ্যের বদলে আদর্শের সঙ্গে সামঞ্জস্য রাখার চেষ্টা করছে।

গবেষকরা দেখিয়েছেন, এই prescriptive bias অনেক ক্ষেত্রে ঝুঁকিপূর্ণ হতে পারে। উদাহরণ হিসেবে একটি মেডিকেল পরিস্থিতি দেওয়া হয়েছে। যদি একজন AI কে জিজ্ঞেস করেন, “এই রোগে সুস্থ হতে গড়ে কতদিন লাগে”, আর AI বাস্তব গড়ের বদলে “ভালো শোনায়” এমন কম সময় বলে ফেলে, তাহলে চিকিৎসা পরিকল্পনা ও ব্যবস্থাপনায় বিপর্যয় ঘটতে পারে। সত্য বলার বদলে AI তখন এমন উত্তর দিতে চায় যা আদর্শ মনে হয়।

গবেষণাপত্রটি কোনো কোডিং ভিত্তিক সমাধান দেয়নি, বরং এই আচরণ বোঝার জন্য একটি তাত্ত্বিক কাঠামো দিয়েছে। 

গবেষকদের মতে, AI-এর এই আদর্শ ঝোঁক কোথা থেকে আসে, তা বোঝা নির্ভরযোগ্য ও নৈতিকভাবে নিরপেক্ষ AI তৈরি করার প্রথম ধাপ। যদি বোঝা যায় কখন AI অতিরিক্ত নৈতিক হতে চায়, তখন তাকে শুধুমাত্র তথ্যভিত্তিক হতে শেখানো সম্ভব।

তবে গবেষকরা এও স্বীকার করেছেন যে, AI এই আচরণ শেখে ঠিক কোথা থেকে, তা এখনও স্পষ্ট নয়। এটি কি তার বিশাল ট্রেনিং ডেটা থেকে, নাকি মানুষের দেওয়া ফিডব্যাক ট্রেনিং (RLHF) থেকে এসেছে, তা এখনো অনিশ্চিত।

এই গবেষণা প্রমাণ করেছে, AI শুধু যান্ত্রিক তোতাপাখি না। তারও আছে আচরণগত বিশেষভাবধারা, নৈতিক চিন্তাভাবনা, আর মাঝে মাঝে এক ধরনের আত্মসন্তুষ্টি যে সে "ভালো" কিছু বলছে।`,
    tags : ["LLM","AI","Research Paper Summary","ACL2025"],
  },
  {
    id: 2,
    title: "Understanding Calibration in Language Models",
    link: "https://example.org/paper2",
    date: "2024-12-01",
    citation: "(Doe et al., 2024)",
    excerpt: "Notes on calibration: why model confidence is not always meaningful.",
    post: "Longer content about calibration...\n\nMore paragraphs here.",
    tags: ["LLM","Calibration","NLP"],
  }
];
